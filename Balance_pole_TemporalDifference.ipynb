{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### posible action : \n",
    "- left , right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enviroment space : \n",
    "- position of cart\n",
    "- velocity of cart\n",
    "- angle of the pole to the verticle\n",
    "- angular velocity in which the pole is moving\n",
    "\n",
    "> low : give lower bounds of 4 values that make up the observation space (individual state variable cannot have values below these)\n",
    "\n",
    "> high : give upper bounds of 4 values that make up the observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretize the state space :\n",
    "> so we can apply Q_Learning to bounded space \n",
    "> (this is a technique to reduce dimentionality Q_value computation)\n",
    "\n",
    "- position of cart (Left , Right)\n",
    "> 1: ignoring this variable completely in our state space\n",
    "- velocity of cart\n",
    "> 1: ignoring this variable completely in our state space\n",
    "- angle of the pole to the verticle\n",
    "- angular velocity in which the pole is moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## two first 1 refer to ignoring 2 dimention of state space\n",
    "## 6 bucket refer to 6 discrete intervals\n",
    "NUM_BUCKETS  = ( 1 , 1 , 6 , 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTION   = env.action_space.n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_BOUNDS = list(zip(env.observation_space.low,env.observation_space.high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### redefine some of bounds in order to further limit our state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-4.8, 4.8),\n",
       " [-0.5, 0.5],\n",
       " (-0.41887903, 0.41887903),\n",
       " [-0.8726646259971648, 0.8726646259971648]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STATE_BOUNDS[1] = [-.5 , .5]\n",
    "STATE_BOUNDS[3] = [-math.radians(50) , math.radians(50)]\n",
    "\n",
    "STATE_BOUNDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize Q_Table :\n",
    "\n",
    "#### num_states × num_actions = (1,1,3,6)×2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of Q_Table : (1, 1, 6, 3, 2)\n",
      "\n",
      "[[[[[0. 0.]\n",
      "    [0. 0.]\n",
      "    [0. 0.]]\n",
      "\n",
      "   [[0. 0.]\n",
      "    [0. 0.]\n",
      "    [0. 0.]]\n",
      "\n",
      "   [[0. 0.]\n",
      "    [0. 0.]\n",
      "    [0. 0.]]\n",
      "\n",
      "   [[0. 0.]\n",
      "    [0. 0.]\n",
      "    [0. 0.]]\n",
      "\n",
      "   [[0. 0.]\n",
      "    [0. 0.]\n",
      "    [0. 0.]]\n",
      "\n",
      "   [[0. 0.]\n",
      "    [0. 0.]\n",
      "    [0. 0.]]]]]\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros(NUM_BUCKETS+(NUM_ACTION,))\n",
    "print(\"shape of Q_Table : {}\\n\".format(q_table.shape))\n",
    "print(q_table)\n",
    "\n",
    "### first 4 dimention refer to states\n",
    "### last dimention refer to actions for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we initially want to explore state space in order to fill up the Q-value in Q_Table\n",
    "- we use an explanatory rate of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORATION_RATE_MIN = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE_MIN =  0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper func. to decay exploration rate  &  learnin rate over time\n",
    "- not too fast\n",
    "- we dont miss any maximums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explore_rete(t):\n",
    "    return max(EXPLORATION_RATE_MIN,min(1,1.0-math.log10((t+1)/25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rete(t):\n",
    "    return max(LEARNING_RATE_MIN,min(0.5,1.0-math.log10((t+1)/25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- explore the sample space at random based on the explore_rate\n",
    "- or we can choose to stick with the known and perform that action that get us to the highest Q_Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_function(state,explore_rate):\n",
    "    if random.random() < explore_rate :\n",
    "        action = env.action_space.sample()\n",
    "    else :\n",
    "        action = np.argmax(q_table[state])\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discritize :\n",
    "- helper func. input   : continuous state\n",
    "- helper func. output : discritized , bucketized version\n",
    "\n",
    "##### iterate through all 4 state variables one at a time\n",
    "\n",
    "- if the state is beyond the lower bounds , set it to the smallest bucket\n",
    "- if the state is beyond the upper bounds , set it to the largest bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_bucket(state):\n",
    "    bucket_indices       = []\n",
    "    \n",
    "    for i in range(len(state)):\n",
    "        ### less than lower bound\n",
    "        if state[i]     <= STATE_BOUNDS[i][0]:\n",
    "            bucket_index = 0\n",
    "        \n",
    "        ### more than upper bound\n",
    "        elif state[i]   >= STATE_BOUNDS[i][1]:\n",
    "            bucket_index = NUM_BUCKETS[i]-1\n",
    "            \n",
    "        else :\n",
    "            bound_width  = STATE_BOUNDS[i][1] - STATE_BOUNDS[i][0]\n",
    "            \n",
    "            offset       = (NUM_BUCKETS[i]-1) * STATE_BOUNDS[i][0] / bound_width\n",
    "            scaling      = (NUM_BUCKETS[i]-1) / bound_width\n",
    "            \n",
    "            bucket_index = int(round(scaling*state[i]-offset))\n",
    "            \n",
    "        bucket_indices.append(bucket_index)\n",
    "        \n",
    "    return tuple(bucket_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An episode ends when : \n",
    "- If the pole is not able to balance on the cart (it goes more than 15 degrees from the vertical)\n",
    "- If the cart moves more than 2.4 units on either side \n",
    "- If when the number of discrete time intervals are up\n",
    "\n",
    "##### we'll have every episode run for a maximum of 250 time intervals\n",
    "- render enviroment for each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation():\n",
    "    ### remember these values be large initially and decay over time\n",
    "    learning_rate   = get_learning_rete(0)\n",
    "    explore_rate    = get_explore_rete(0)\n",
    "    \n",
    "    ### gamma : future rewards are almost as important as immediate rewards\n",
    "    discount_factor = 0.99\n",
    "    ### how long we can balance the pole on the cart (for inst. 200 time => streak:1)\n",
    "    num_streaks     = 0\n",
    "    \n",
    "    for episod in range(1000):\n",
    "        observ      = env.reset()\n",
    "        \n",
    "        ### discretize observations (states)\n",
    "        state_0     = state_to_bucket(observ)\n",
    "        \n",
    "        for t in range(250):\n",
    "            env.render()\n",
    "            \n",
    "            action  = select_action_function(state_0,explore_rate)\n",
    "            \n",
    "            observ , reward , done , _ = env.step(action)\n",
    "            \n",
    "            ### each time slot the observation into discrete states(buckets) \n",
    "            state   = state_to_bucket(observ)\n",
    "            \n",
    "            ### best state-action combination \n",
    "            best_q  = np.max(q_table(state))\n",
    "            \n",
    "            ### use Temporal difference method\n",
    "            q_table[state_0 + (action,)] += learning_rate * (reward+discount_factor*(best_q)-q_table[state_0+(action,)])\n",
    "            \n",
    "            state_0 = state\n",
    "            \n",
    "            print(\"\\nEpisode = %d\" % episod)\n",
    "            print(\"t = %d\" % t)\n",
    "            print(\"Action: %d\" % action)\n",
    "            print(\"State: %s\" % str(state))\n",
    "            print(\"Reward: %f\" % reward)\n",
    "            print(\"Best Q: %f\" % best_q)\n",
    "            print(\"Explore rate: %f\" % explore_rate)\n",
    "            print(\"Learning rate %f\" % learning_rate)\n",
    "            print(\"Streaks: %d\" % num_streaks)\n",
    "            \n",
    "            print(\"\")\n",
    "            \n",
    "            if done:\n",
    "                print(\"Episode %d finished after %f time steps\" % (episod,t))\n",
    "                \n",
    "                ### balance the pole on the cart for a fairly long time (one streak)\n",
    "                if (t>=199):\n",
    "                    num_streaks += 1\n",
    "                    \n",
    "                else:\n",
    "                    num_streaks  = 0\n",
    "                    \n",
    "                break\n",
    "        if num_streaks > 120:\n",
    "            break\n",
    "                \n",
    "        explore_rate  = get_explore_rete(episod)\n",
    "        learning_rate = get_learning_rete(episod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.3979400086720375"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
