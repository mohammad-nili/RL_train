{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviroments :\n",
    "\n",
    "> CartPole-v0\n",
    "\n",
    "> Acrobot-v1\n",
    "\n",
    "> MountainCar-v0\n",
    "\n",
    "> FrozenLake-v0\n",
    ">> S:Start  |  F:Frozen  |   H:Hole  |G:Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### posible action : \n",
    "- left , right , Top , bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enviroment space : \n",
    "- 4*4 gride (16 tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize Learning Rate :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize Discount Factor :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize Q_Table :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1, 1, 1, 1],\n",
       " 1: [1, 1, 1, 1],\n",
       " 2: [1, 1, 1, 1],\n",
       " 3: [1, 1, 1, 1],\n",
       " 4: [1, 1, 1, 1],\n",
       " 5: [1, 1, 1, 1],\n",
       " 6: [1, 1, 1, 1],\n",
       " 7: [1, 1, 1, 1],\n",
       " 8: [1, 1, 1, 1],\n",
       " 9: [1, 1, 1, 1],\n",
       " 10: [1, 1, 1, 1],\n",
       " 11: [1, 1, 1, 1],\n",
       " 12: [1, 1, 1, 1],\n",
       " 13: [1, 1, 1, 1],\n",
       " 14: [1, 1, 1, 1],\n",
       " 15: [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### key is current state\n",
    "\n",
    "q_table = dict([x,[1,1,1,1]] for x in range(16))\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper func. to choose the next action based on the current observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(observ):\n",
    "    return np.argmax(q_table[observ]) ## action with highest Q-Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we know nothing about enviroment , lets explore :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each reset of enviroment can be considered to be one episode\n",
    "- an episode is start of a new task , and this task is seen through until the end\n",
    "- we consider a maximum of 10,000 time intervals for each episode\n",
    "- at start of each episode we do not have any value of prev_observ & prev_action\n",
    "- run 2500 time steps in each episode\n",
    "- an episode is considered complete\n",
    "- r=1.0 implies that we reach the goal tile\n",
    "- r=0.0 implies we fail\n",
    "> if we reach the goal (retrieve frisbee) or if we fall in hole and plummet to our death or if we run continuously for 2500 time instance without reaching the goal and whithout dying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> in SARSA method we update Q_Value for the previous state-action combination after executing the action and collecting the actual reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    observ        = env.reset()\n",
    "    action        = choose_action(observ)\n",
    "    prev_observ   = None\n",
    "    prev_action   = None\n",
    "    \n",
    "    t             = 0\n",
    "    \n",
    "    for t in range(2500):\n",
    "        env.render()\n",
    "        \n",
    "        ### observation is next state because of our action\n",
    "        ### reward is for that particular action\n",
    "        observ , reward , done , info = env.step(action)\n",
    "        action    = choose_action(observ)\n",
    "        \n",
    "        ### update Q_Values in Q_Table provided we had a previous state\n",
    "        if not prev_observ is None :\n",
    "            \n",
    "            q_old = q_table[prev_observ][prev_action]\n",
    "            q_new = q_old\n",
    "            \n",
    "            ### if the episode is complete we calc new Q_value for previous state-action comb.\n",
    "            ### this is simplified formula that only contain reward for current action \n",
    "            ### if the episode end there is no future reward to discount\n",
    "            if done :\n",
    "                q_new += alpha * (reward - q_old)\n",
    "            \n",
    "            ### if the episode isn't done yet we use SARSA \n",
    "            ### Remember SARSA uses Actual reward obtained to calc. Q_Value for prev state\n",
    "            else :\n",
    "                q_new += alpha * (reward + gamma * q_table[observ][action]-q_old)\n",
    "                \n",
    "            \n",
    "            new_table              = q_table[prev_observ]\n",
    "            new_table[prev_action] = q_new\n",
    "            q_table[prev_observ]   = new_table\n",
    "            #___________________ or___________________________!\n",
    "            #q_table[prev_observ][prev_action] = q_new\n",
    "        \n",
    "        prev_observ = observ\n",
    "        prev_action = action\n",
    "        \n",
    "        if done :\n",
    "            print(\"Episode {} finished after {} timesteps with r={}.\".format(i,t,reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![dis.png](https://i.postimg.cc/hG0FmYpf/dis.png)](https://postimg.cc/KK4sS0X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color=red>also the action says left but with small probability some other action might be taken (for exploration) so to goes down</font></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The last update that we made to particular Q_Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29651635792390385,\n",
       " 0.32891410132554755,\n",
       " 0.9941406608565958,\n",
       " 0.3211242719374764]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully populated Q_Table :\n",
    "- Q_Values computed using SARSA method for every state-action combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.616709988569774,\n",
       "  0.15437045295331192,\n",
       "  0.1647438033245746,\n",
       "  0.16674122440969114],\n",
       " 1: [0.09741929108316053,\n",
       "  0.09723723932205154,\n",
       "  0.09096514983655313,\n",
       "  0.6861603434743044],\n",
       " 2: [0.11728984582734778,\n",
       "  0.12773695601909563,\n",
       "  0.0994933777194287,\n",
       "  0.7323632235970641],\n",
       " 3: [0.09651028021773994,\n",
       "  0.12005512575970613,\n",
       "  0.12944454219970558,\n",
       "  0.7258811585911],\n",
       " 4: [0.6453960767075267,\n",
       "  0.12954815999999997,\n",
       "  0.1032919938882575,\n",
       "  0.09600963782156191],\n",
       " 5: [1, 1, 1, 1],\n",
       " 6: [0.007016734361078387,\n",
       "  0.005992242425440991,\n",
       "  0.48449702394137384,\n",
       "  0.006903589158444359],\n",
       " 7: [1, 1, 1, 1],\n",
       " 8: [0.09623518823465951,\n",
       "  0.11015373425147809,\n",
       "  0.12941835706015267,\n",
       "  0.6749081330801756],\n",
       " 9: [0.10662202834920921,\n",
       "  0.7990166702474116,\n",
       "  0.10392309758111536,\n",
       "  0.11297635549382402],\n",
       " 10: [0.6876114871375615,\n",
       "  0.040782636827680935,\n",
       "  0.05798380499977701,\n",
       "  0.06102741687492574],\n",
       " 11: [1, 1, 1, 1],\n",
       " 12: [1, 1, 1, 1],\n",
       " 13: [0.19813718308674946,\n",
       "  0.21591359999999996,\n",
       "  0.9503445002033969,\n",
       "  0.23818018883535969],\n",
       " 14: [0.29651635792390385,\n",
       "  0.32891410132554755,\n",
       "  0.9941406608565958,\n",
       "  0.3211242719374764],\n",
       " 15: [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attachment :\n",
    "\n",
    "[Project Malmo](https://www.microsoft.com/en-us/research/project/project-malmo/)\n",
    "\n",
    "[Deepmind Lab](https://deepmind.com/research/publications/2019/deepmind-lab)\n",
    "\n",
    "[VIZDOOM](http://vizdoom.cs.put.edu.pl/)\n",
    "\n",
    "[gym](https://gym.openai.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
